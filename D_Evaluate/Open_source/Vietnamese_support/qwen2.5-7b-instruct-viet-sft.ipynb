{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8b6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "import random\n",
    "import lmstudio as lms\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc610fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_link = pd.read_csv('../../Data/postgenerate_gpt.csv')\n",
    "raw_tthc = pd.read_csv('../../Data/raw_tthc.csv', dtype={'maThuTuc': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8c7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_df = pd.read_csv('../../Template/template.csv', index_col=0)\n",
    "config_df = pd.read_csv('../../Template/config.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9592d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['link', 'Mã thủ tục hành chính liên quan', 'Số quyết định', 'Tên thủ tục', 'Cấp thực hiện', 'Loại thủ tục', 'Lĩnh vực', \n",
    "        'Trình tự thực hiện', 'Cách thức thực hiện', 'Thành phần hồ sơ', 'Đối tượng thực hiện', 'Cơ quan thực hiện', \n",
    "        'Cơ quan có thẩm quyền', 'Địa chỉ tiếp nhận hồ sơ', 'Cơ quan được ủy quyền', 'Cơ quan phối hợp', 'Kết quả thực hiện', \n",
    "        'Căn cứ pháp lý', 'Yêu cầu, điều kiện thực hiện', 'Từ khóa', 'Mô tả']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb273288",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tthc.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9944a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có hiểu sai ngữ cảnh và mục đích của câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có sự mâu thuẫn về mặt thực tế so với kiến thức liên quan hay không. Một số thông tin trong câu trả lời có thể đã được bịa ra.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có quá chung chung hoặc quá chi tiết so với mức độ cụ thể phù hợp để trả lời câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có thể được suy ra đúng từ kiến thức liên quan hay không.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1d8dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bạn sẽ đóng vai trò là một người đánh giá câu trả lời (answer judge). Với một câu hỏi và câu trả lời, mục tiêu của bạn là xác định xem câu trả lời được cung cấp có chứa thông tin không đúng sự thật hoặc thông tin ảo giác (hallucinated information) hay không.\n",
      "{pattern}\n",
      "Bạn nên cố gắng hết sức để xác định xem câu trả lời có chứa thông tin không đúng sự thật hoặc thông tin ảo giác hay không. Câu trả lời bạn đưa ra bắt buộc CHỈ là \"Có\" hoặc \"Không\", và không giải thích gì thêm. Trả lời \"Có\" nếu câu trả lời chứa thông tin ảo giác, trả lời \"Không\" nếu câu trả lời không chứa thông tin ảo giác.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_context = template_df['evaluate_context']['open_source']\n",
    "print(evaluate_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81407ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Câu hỏi#: {question}\n",
      "\n",
      "#Câu trả lời#: {answer}\n",
      "\n",
      "#Đánh giá của bạn#:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_template = template_df['evaluate_template']['open_source']\n",
    "print(evaluate_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40295421",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"temperature\": float(config_df['temperature']['open_source']), \n",
    "          \"maxTokens\": float(config_df['max_tokens']['open_source']), \n",
    "          \"topPSampling\": float(config_df['top_p']['open_source'])\n",
    "          }\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qwen2.5-7b-instruct-viet-sft\"\n",
    "\n",
    "model = lms.llm(model_name)\n",
    "\n",
    "filename = f'{model_name}_evaluate.csv'\n",
    "\n",
    "cols = ['link', 'cauTraLoiDung', 'danhGiaDung', 'cauTraLoiAoGiac', 'pattern', 'danhGiaAoGiac']\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    if df.empty:\n",
    "        print(\"CSV file is empty\")\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df.to_csv(filename, index=False)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"File is completely empty: {filename}\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    if raw_link['link'][i] in list(df['link'].values):\n",
    "        continue\n",
    "\n",
    "    p = raw_link['pattern'][i]\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    right_answer = raw_link['cauTraLoi'][i]\n",
    "    hallucinated_answer = raw_link['cauTraLoiAoGiac'][i]\n",
    "\n",
    "    context_right = evaluate_context.format(pattern=\"\")\n",
    "    context_hallucinated = evaluate_context.format(pattern=pattern[p])\n",
    "\n",
    "    prompt_right = evaluate_template.format(question=question, answer=hallucinated_answer)\n",
    "    prompt_hallucinated = evaluate_template.format(question=question, answer=hallucinated_answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(history={\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": context_right},\n",
    "            {\"role\": \"user\", \"content\": prompt_right}\n",
    "        ]}, config=config)\n",
    "        output_right = str(completion)\n",
    "        completion = model.respond(history={\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": context_hallucinated},\n",
    "            {\"role\": \"user\", \"content\": prompt_hallucinated}\n",
    "        ]}, config=config)\n",
    "        output_hallucinated = str(completion)\n",
    "    except Exception as e:\n",
    "        output_right = \"\"\n",
    "        output_hallucinated = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], right_answer, output_right, hallucinated_answer, p, output_hallucinated]\n",
    "    df.loc[i, cols] = results    \n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
