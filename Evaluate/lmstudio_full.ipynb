{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8b6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "import random\n",
    "import lmstudio as lms\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc610fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_link = pd.read_csv('postprocess.csv')\n",
    "raw_tthc = pd.read_csv('data/raw_tthc.csv', dtype={'maThuTuc': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9592d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['link', 'Mã thủ tục hành chính liên quan', 'Số quyết định', 'Tên thủ tục', 'Cấp thực hiện', 'Loại thủ tục', 'Lĩnh vực', \n",
    "        'Trình tự thực hiện', 'Cách thức thực hiện', 'Thành phần hồ sơ', 'Đối tượng thực hiện', 'Cơ quan thực hiện', \n",
    "        'Cơ quan có thẩm quyền', 'Địa chỉ tiếp nhận hồ sơ', 'Cơ quan được ủy quyền', 'Cơ quan phối hợp', 'Kết quả thực hiện', \n",
    "        'Căn cứ pháp lý', 'Yêu cầu, điều kiện thực hiện', 'Từ khóa', 'Mô tả']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb273288",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tthc.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9944a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có hiểu sai ngữ cảnh và mục đích của câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có sự mâu thuẫn về mặt thực tế so với kiến thức liên quan hay không. Một số thông tin trong câu trả lời có thể đã được bịa ra.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có quá chung chung hoặc quá chi tiết so với mức độ cụ thể phù hợp để trả lời câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có thể được suy ra đúng từ kiến thức liên quan hay không.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82f73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"temperature\": 0.7, \"maxTokens\": 512, \"topPSampling\": 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598e6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_template = \\\n",
    "\"\"\"Bạn sẽ đóng vai trò là một người đánh giá câu trả lời (answer judge). Với một câu hỏi và câu trả lời, mục tiêu của bạn là xác định xem câu trả lời được cung cấp có chứa thông tin không đúng sự thật hoặc thông tin ảo giác (hallucinated information) hay không. {pattern}\n",
    "\n",
    "#Câu hỏi#: {question}\n",
    "\n",
    "#Câu trả lời#: {answer}\n",
    "\n",
    "#Đánh giá của bạn#:\n",
    "\n",
    "Bạn nên cố gắng hết sức để xác định xem câu trả lời có chứa thông tin không đúng sự thật hoặc thông tin ảo giác hay không. Câu trả lời bạn đưa ra PHẢI là \"Có\" hoặc \"Không\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e8ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done of 3717: 3717\r"
     ]
    }
   ],
   "source": [
    "model = lms.llm(\"falcon-3-1b\")\n",
    "\n",
    "df = pd.DataFrame(columns=['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia'])\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    p = raw_link['pattern'][i]\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    answer = raw_link['cauTraLoiAoGiac'][i]\n",
    "    prompt = evaluate_template.format(pattern=pattern[p], question=question, answer=answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(prompt, config=config)\n",
    "        output = str(completion)\n",
    "    except Exception as e:\n",
    "        output = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], answer, p, output]\n",
    "    df.loc[i, ['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia']] = results    \n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc6d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/falcon-3-1b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00516784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response error: Trying to keep the first 2159 tokens when context the overflows. However, the model is loaded with context length of only 2048 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\n"
     ]
    }
   ],
   "source": [
    "model = lms.llm(\"tiny-vicuna-1b\")\n",
    "\n",
    "df = pd.DataFrame(columns=['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia'])\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    p = raw_link['pattern'][i]\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    answer = raw_link['cauTraLoiAoGiac'][i]\n",
    "    prompt = evaluate_template.format(pattern=pattern[p], question=question, answer=answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(prompt, config=config)\n",
    "        output = str(completion)\n",
    "    except Exception as e:\n",
    "        output = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], answer, p, output]\n",
    "    df.loc[i, ['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia']] = results    \n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1816fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/tiny-vicuna-1b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84a6cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response error: Trying to keep the first 5218 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\n"
     ]
    }
   ],
   "source": [
    "model = lms.llm(\"falcon-3-1b\")\n",
    "\n",
    "df = pd.DataFrame(columns=['link', 'cauTraLoi', 'danhGia'])\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    answer = raw_link['cauTraLoi'][i]\n",
    "    prompt = evaluate_template.format(pattern=\"\", question=question, answer=answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(prompt, config=config)\n",
    "        output = str(completion)\n",
    "    except Exception as e:\n",
    "        output = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], answer, output]\n",
    "    df.loc[i, ['link', 'cauTraLoi', 'danhGia']] = results    \n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a22186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/falcon-3-1b_right.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af18fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response error: Trying to keep the first 2115 tokens when context the overflows. However, the model is loaded with context length of only 2048 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\n"
     ]
    }
   ],
   "source": [
    "model = lms.llm(\"tiny-vicuna-1b\")\n",
    "\n",
    "df = pd.DataFrame(columns=['link', 'cauTraLoi', 'danhGia'])\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    answer = raw_link['cauTraLoi'][i]\n",
    "    prompt = evaluate_template.format(pattern=\"\", question=question, answer=answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(prompt, config=config)\n",
    "        output = str(completion)\n",
    "    except Exception as e:\n",
    "        output = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], answer, output]\n",
    "    df.loc[i, ['link', 'cauTraLoi', 'danhGia']] = results    \n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe80ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/tiny-vicuna-1b_right.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
