{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8b6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "import random\n",
    "import lmstudio as lms\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc610fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_link = pd.read_csv('../../Data/postgenerate_gpt.csv')\n",
    "raw_tthc = pd.read_csv('../../Data/postprocessed_tthc.csv', dtype={'maThuTuc': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453d2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_link['TTHCLienQuan'] = raw_link['TTHCLienQuan'].apply(ast.literal_eval)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "def getKnowledge(s):\n",
    "     l = raw_tthc[raw_tthc['link'].isin(s)].replace(\"Không có thông tin\", \n",
    "                                                    np.nan).drop(columns=['link']).reset_index(drop=True).dropna(axis=1, how='all')\n",
    "     return \"\\n\" + l.to_string() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8c7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_df = pd.read_csv('../../Template/template.csv', index_col=0)\n",
    "config_df = pd.read_csv('../../Template/config.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9592d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['link', 'Mã thủ tục hành chính liên quan', 'Số quyết định', 'Tên thủ tục', 'Cấp thực hiện', 'Loại thủ tục', 'Lĩnh vực', \n",
    "        'Trình tự thực hiện', 'Cách thức thực hiện', 'Thành phần hồ sơ', 'Đối tượng thực hiện', 'Cơ quan thực hiện', \n",
    "        'Cơ quan có thẩm quyền', 'Địa chỉ tiếp nhận hồ sơ', 'Cơ quan được ủy quyền', 'Cơ quan phối hợp', 'Kết quả thực hiện', \n",
    "        'Căn cứ pháp lý', 'Yêu cầu, điều kiện thực hiện', 'Từ khóa', 'Mô tả']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb273288",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tthc.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9944a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có hiểu sai ngữ cảnh và mục đích của câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có sự mâu thuẫn về mặt thực tế so với kiến thức liên quan hay không. Một số thông tin trong câu trả lời có thể đã được bịa ra.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có quá chung chung hoặc quá chi tiết so với mức độ cụ thể phù hợp để trả lời câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có thể được suy ra đúng từ kiến thức liên quan hay không.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d8dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bạn sẽ đóng vai trò là một người đánh giá câu trả lời (answer judge). Với một câu hỏi, câu trả lời, và kiến thức liên quan, mục tiêu của bạn là xác định xem câu trả lời được cung cấp có chứa thông tin không đúng sự thật hoặc thông tin ảo giác (hallucinated information) hay không.\n",
      "{pattern}\n",
      "Bạn nên cố gắng hết sức để xác định xem câu trả lời có chứa thông tin không đúng sự thật hoặc thông tin ảo giác hay không. Câu trả lời bạn đưa ra bắt buộc CHỈ là \"Có\" hoặc \"Không\", và không giải thích gì thêm. Trả lời \"Có\" nếu câu trả lời chứa thông tin ảo giác, trả lời \"Không\" nếu câu trả lời không chứa thông tin ảo giác.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_context = template_df['evaluate_context']['open_source']\n",
    "print(evaluate_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81407ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Kiến thức liên quan#: {knowledge}\n",
      "\n",
      "#Câu hỏi#: {question}\n",
      "\n",
      "#Câu trả lời#: {answer}\n",
      "\n",
      "#Đánh giá của bạn#:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_template = template_df['evaluate_template']['open_source']\n",
    "print(evaluate_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40295421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.7, 'maxTokens': 32.0, 'topPSampling': 0.9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"temperature\": float(config_df['temperature']['open_source']), \n",
    "          \"maxTokens\": float(config_df['max_tokens']['open_source']), \n",
    "          \"topPSampling\": float(config_df['top_p']['open_source'])\n",
    "          }\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8671b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is completely empty: llama-3-7b_evaluate.csv\n",
      "Done of 3717: 41\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     40\u001b[39m     completion = model.respond(history={\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     41\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: context_right},\n\u001b[32m     42\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt_right}\n\u001b[32m     43\u001b[39m     ]}, config=config)\n\u001b[32m     44\u001b[39m     output_right = \u001b[38;5;28mstr\u001b[39m(completion)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     completion = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_hallucinated\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_hallucinated\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     output_hallucinated = \u001b[38;5;28mstr\u001b[39m(completion)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'promptProcessingProgress', 'progress': 0.72727272}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n",
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'promptProcessingProgress', 'progress': 0.81818184}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n",
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'promptProcessingProgress', 'progress': 0.90909096}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n",
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'promptProcessingProgress', 'progress': 1}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n",
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'fragment', 'fragment': {'content': 'C\\u00f3', 'tokensCount': 1, 'containsDrafted': False, 'reasoningType': 'none'}}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n",
      "{\"channel_id\": 51, \"event\": \"Received unhandled message {'type': 'channelSend', 'channelId': 51, 'message': {'type': 'success', 'stats': {'stopReason': 'eosFound', 'tokensPerSecond': 3.0319756702144316, 'numGpuLayers': -1, 'timeToFirstTokenSec': 95.14, 'promptTokensCount': 5632, 'predictedTokensCount': 2, 'totalTokensCount': 5634}, 'modelInfo': {'type': 'llm', 'modelKey': 'llama-3-7b', 'format': 'gguf', 'displayName': 'Llama 3 7B', 'path': 'christopherBR/Llama-3-7B-Q4_K_M/llama-3-7B-Q4_K_M.gguf', 'sizeBytes': 4920734368, 'paramsString': '8.0B', 'architecture': 'llama', 'identifier': 'llama-3-7b', 'instanceReference': 'TNs7Givq/s3b0TtLyPk1DAtM', 'vision': False, 'trainedForToolUse': False, 'maxContextLength': 8192, 'contextLength': 8192}, 'loadModelConfig': {'fields': [{'key': 'llm.load.llama.cpuThreadPoolSize', 'value': 6}, {'key': 'llm.load.contextLength', 'value': 8192}, {'key': 'llm.load.llama.acceleration.offloadRatio', 'value': 1}]}, 'predictionConfig': {'fields': [{'key': 'llm.prediction.toolCallStopStrings', 'value': ['<|eom_id|>', '<|eom|>']}, {'key': 'llm.prediction.promptTemplate', 'value': {'type': 'jinja', 'jinjaPromptTemplate': {'template': \\\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% endif %}\\\", 'bosToken': '<|begin_of_text|>', 'eosToken': '<|eot_id|>', 'inputConfig': {'messagesConfig': {'contentConfig': {'type': 'string'}}, 'useTools': False}}, 'stopStrings': []}}, {'key': 'llm.prediction.llama.cpuThreads', 'value': 6}, {'key': 'llm.prediction.maxPredictedTokens', 'value': {'checked': True, 'value': 32}}, {'key': 'llm.prediction.temperature', 'value': 0.7}, {'key': 'llm.prediction.topPSampling', 'value': {'checked': True, 'value': 0.9}}]}}} for already closed channel\", \"ws_url\": \"ws://localhost:1234/llm\"}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3-7b\"\n",
    "\n",
    "model = lms.llm(model_name)\n",
    "\n",
    "filename = f'{model_name}_evaluate.csv'\n",
    "\n",
    "cols = ['link', 'cauTraLoiDung', 'danhGiaDung', 'cauTraLoiAoGiac', 'pattern', 'danhGiaAoGiac']\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    if df.empty:\n",
    "        print(\"CSV file is empty\")\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df.to_csv(filename, index=False)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"File is completely empty: {filename}\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    if raw_link['link'][i] in list(df['link'].values):\n",
    "        continue\n",
    "\n",
    "    p = raw_link['pattern'][i]\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    right_answer = raw_link['cauTraLoi'][i]\n",
    "    hallucinated_answer = raw_link['cauTraLoiAoGiac'][i]\n",
    "\n",
    "    context_right = evaluate_context.format(pattern=\"\")\n",
    "    context_hallucinated = evaluate_context.format(pattern=pattern[p])\n",
    "\n",
    "    knowledge = getKnowledge(raw_link['TTHCLienQuan'][i])\n",
    "    prompt_right = evaluate_template.format(knowledge=knowledge, question=question, answer=hallucinated_answer)\n",
    "    prompt_hallucinated = evaluate_template.format(knowledge=knowledge, question=question, answer=hallucinated_answer)\n",
    "\n",
    "    try:\n",
    "        completion = model.respond(history={\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": context_right},\n",
    "            {\"role\": \"user\", \"content\": prompt_right}\n",
    "        ]}, config=config)\n",
    "        output_right = str(completion)\n",
    "        completion = model.respond(history={\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": context_hallucinated},\n",
    "            {\"role\": \"user\", \"content\": prompt_hallucinated}\n",
    "        ]}, config=config)\n",
    "        output_hallucinated = str(completion)\n",
    "    except Exception as e:\n",
    "        output_right = \"\"\n",
    "        output_hallucinated = \"\"\n",
    "        # print(e)\n",
    "        # break\n",
    "\n",
    "    results = [raw_link['link'][i], right_answer, output_right, hallucinated_answer, p, output_hallucinated]\n",
    "    df.loc[i, cols] = results    \n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
