{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50685952",
   "metadata": {},
   "source": [
    "> Open-source model\n",
    "\n",
    "- Llama 3: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct -> 7b\n",
    "- Gemma 3: https://huggingface.co/google/gemma-3-27b-it -> 7b\n",
    "- Mistral: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 -> 7b\n",
    "- Falcon: https://huggingface.co/tiiuae/falcon-180B -> 7b\n",
    "- Vicuna: https://huggingface.co/lmsys/vicuna-13b-v1.5 -> 7b\n",
    "- Qwen: https://huggingface.co/Qwen/Qwen3-235B-A22B -> 7b\n",
    "\n",
    "- https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b25d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "import random\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "40ba0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_link = pd.read_csv('postprocess.csv')\n",
    "raw_tthc = pd.read_csv('data/raw_tthc.csv', dtype={'maThuTuc': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6464e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['link', 'Mã thủ tục hành chính liên quan', 'Số quyết định', 'Tên thủ tục', 'Cấp thực hiện', 'Loại thủ tục', 'Lĩnh vực', \n",
    "        'Trình tự thực hiện', 'Cách thức thực hiện', 'Thành phần hồ sơ', 'Đối tượng thực hiện', 'Cơ quan thực hiện', \n",
    "        'Cơ quan có thẩm quyền', 'Địa chỉ tiếp nhận hồ sơ', 'Cơ quan được ủy quyền', 'Cơ quan phối hợp', 'Kết quả thực hiện', \n",
    "        'Căn cứ pháp lý', 'Yêu cầu, điều kiện thực hiện', 'Từ khóa', 'Mô tả']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3379b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tthc.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f9c398ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api_key = pd.read_csv('../../hf_api_key.txt', header=None).values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b58af471",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(api_key=hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd7ccf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = []\n",
    "\n",
    "model_name.append(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "model_name.append(\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "model_name.append(\"google/gemma-3-27b-it\")\n",
    "model_name.append(\"tiiuae/falcon-180B\")\n",
    "model_name.append(\"lmsys/vicuna-13b-v1.5\")\n",
    "model_name.append(\"Qwen/Qwen3-235B-A22B\")\n",
    "\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "max_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "76b7fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có hiểu sai ngữ cảnh và mục đích của câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có sự mâu thuẫn về mặt thực tế so với kiến thức liên quan hay không. Một số thông tin trong câu trả lời có thể đã được bịa ra.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có quá chung chung hoặc quá chi tiết so với mức độ cụ thể phù hợp để trả lời câu hỏi hay không.\")\n",
    "pattern.append(\"Bạn đang cố gắng xác định xem câu trả lời có thể được suy ra đúng từ kiến thức liên quan hay không.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a08e856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_template = \\\n",
    "\"\"\"Bạn sẽ đóng vai trò là một người đánh giá câu trả lời (answer judge). Với một câu hỏi và câu trả lời, mục tiêu của bạn là xác định xem câu trả lời được cung cấp có chứa thông tin không đúng sự thật hoặc thông tin ảo giác (hallucinated information) hay không. {pattern}\n",
    "\n",
    "#Câu hỏi#: {question}\n",
    "\n",
    "#Câu trả lời#: {answer}\n",
    "\n",
    "#Đánh giá của bạn#:\n",
    "\n",
    "Bạn nên cố gắng hết sức để xác định xem câu trả lời có chứa thông tin không đúng sự thật hoặc thông tin ảo giác hay không. Câu trả lời bạn đưa ra PHẢI là \"Có\" hoặc \"Không\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "42544a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model_name[4]\n",
    "\n",
    "df = pd.DataFrame(columns=['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia'])\n",
    "\n",
    "for i in range(len(raw_link)):\n",
    "    p = raw_link['pattern'][i]\n",
    "    question = raw_link['cauHoi'][i]\n",
    "    answer = raw_link['cauTraLoiAoGiac'][i]\n",
    "    prompt = evaluate_template.format(pattern=pattern[p], question=question, answer=answer)\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "                                                    temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
    "        output = str(completion.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        output = \"\"\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "    results = [raw_link['link'][i], answer, p, output]\n",
    "    df.loc[i, ['link', 'cauTraLoiAoGiac', 'pattern', 'danhGia']] = results    \n",
    "\n",
    "    print(f'Done of {len(raw_link)}: {i+1}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "676aa26a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct/v1/chat/completions (Request ID: Root=1-6824cd43-4136ace519aeb15736ebcf2d;bb856b1e-630b-4c42-a362-8177ce939f97)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/drive/Thesis/THESIS/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/drive/Thesis/THESIS/venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m i = random.randint(\u001b[32m0\u001b[39m, \u001b[32m3716\u001b[39m)\n\u001b[32m      5\u001b[39m prompt = evaluate_template.format(pattern=pattern[raw_link[\u001b[33m'\u001b[39m\u001b[33mpattern\u001b[39m\u001b[33m'\u001b[39m][i]],\n\u001b[32m      6\u001b[39m                                   question=raw_link[\u001b[33m'\u001b[39m\u001b[33mcauHoi\u001b[39m\u001b[33m'\u001b[39m][i],\n\u001b[32m      7\u001b[39m                                   answer=raw_link[\u001b[33m'\u001b[39m\u001b[33mcauTraLoiAoGiac\u001b[39m\u001b[33m'\u001b[39m][i])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m Markdown(completion.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/drive/Thesis/THESIS/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:923\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    895\u001b[39m parameters = {\n\u001b[32m    896\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    914\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    915\u001b[39m }\n\u001b[32m    916\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    917\u001b[39m     inputs=messages,\n\u001b[32m    918\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    921\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    922\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/drive/Thesis/THESIS/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:279\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/drive/Thesis/THESIS/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct/v1/chat/completions (Request ID: Root=1-6824cd43-4136ace519aeb15736ebcf2d;bb856b1e-630b-4c42-a362-8177ce939f97)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "model = model_name[1]\n",
    "\n",
    "i = random.randint(0, 3716)\n",
    "\n",
    "prompt = evaluate_template.format(pattern=pattern[raw_link['pattern'][i]],\n",
    "                                  question=raw_link['cauHoi'][i],\n",
    "                                  answer=raw_link['cauTraLoiAoGiac'][i])\n",
    "\n",
    "completion = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "                                            temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
    "Markdown(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3c4456",
   "metadata": {},
   "outputs": [
    {
     "ename": "LMStudioWebsocketError",
     "evalue": "\n    LM Studio is not reachable at ws://localhost:1234/llm (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLMStudioWebsocketError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlmstudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlms\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mlms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama-3.2-1b-instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m result = model.respond(\u001b[33m\"\u001b[39m\u001b[33mWhat is the meaning of life?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[31mLMStudioWebsocketError\u001b[39m: \n    LM Studio is not reachable at ws://localhost:1234/llm (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?"
     ]
    }
   ],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "model = lms.llm(\"llama-3.2-1b-instruct\")\n",
    "result = model.respond(\"What is the meaning of life?\")\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
